{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5bc0703b",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network\n",
    "- shallow (one layer) / deep (an x number of hidden layers)\n",
    "- each node is a simple function with a simple job\n",
    "- pattern matching is done through the connection of many nodes to create one powerful function\n",
    "- this function has an understanding of the data's **sequential nature**\n",
    "    - using feedback loops that form a sense of memory\n",
    "    - the sequential nature accounts for the context (the problem word2vec tackles by exploring the 'sliding window'...)\n",
    "- each node is aware of the output from the other node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528d80a0",
   "metadata": {},
   "source": [
    "### other NLP techniques:\n",
    "1. **TF-IDF**\n",
    "    - uses a multi-million-word dictionary object with frequency count (trained on a given corpus)\n",
    "    - compares the occurence of certain words from a document/sentence and creates a sparse-vector (mostly zeros, with values on the dictionary's 'index' where that specific word occured in the given sentence\n",
    "    - calculates the 'importance' of a given word in a given sentence by comparing the total frequency with inverted frequency count (\"how important is this word in this sentence?\")\n",
    "    - the vectors will be **sparse**\n",
    "    \n",
    "2. **word2vec** \n",
    "    - for each given word in the sentence, based on a window (2,3 or more tokens) calculates a vector\n",
    "    - to represent a given sentence, an average value of all the word vectors is calculated\n",
    "    - the vector of the sentence will be **dense** (shorter, no zeros)\n",
    "    \n",
    "3. **doc2vec**\n",
    "    - a more sophisticated way to calculate sentence/document vector (it doesn't lose information by averaging the value)\n",
    "    - the vector of the sentence will be **dense**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4eae0aea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label  \\\n",
      "0   ham   \n",
      "1   ham   \n",
      "2  spam   \n",
      "3   ham   \n",
      "4   ham   \n",
      "\n",
      "                                                                                                  text  \n",
      "0  Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there g...  \n",
      "1                                                                        Ok lar... Joking wif u oni...  \n",
      "2  Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive ...  \n",
      "3                                                    U dun say so early hor... U c already then say...  \n",
      "4                                        Nah I don't think he goes to usf, he lives around here though  \n",
      "X_train (4459,)\n",
      "(1115,)\n",
      "y_train: (4459,)\n",
      "(1115,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-18ccb03ac254>:6: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  data = pd.read_csv('dataset/SMSSpamCollection.csv', sep='delimiter')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('display.max_colwidth', 100)\n",
    "\n",
    "data = pd.read_csv('dataset/SMSSpamCollection.csv', sep='delimiter')\n",
    "messages = pd.DataFrame(columns=['label','text'])\n",
    "messages[['label','text']] = data['v1\\tv2'].str.split('\\t', expand=True)\n",
    "\n",
    "# the labels will be converted from str to int data (less memory consuming)\n",
    "labels = np.where(messages['label'] == 'spam', 1,0)\n",
    "\n",
    "\n",
    "print(messages.head())\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    messages['text'],\n",
    "    labels,\n",
    "    test_size=0.2\n",
    ")\n",
    "\n",
    "print(\"X_train\", X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421e12ee",
   "metadata": {},
   "source": [
    "### preparing data for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef99688b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keras\n",
    "# !conda install keras\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02ca1f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and fit the tokenizer\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "15a19596",
   "metadata": {},
   "outputs": [],
   "source": [
    "### now the tokenizer has built a vocabulary and assign indices to each token\n",
    "### the next thing is to pass each sentence (or a message in our case) to a built-in function \n",
    "    # that converts each token in a text to a unique integer \n",
    "    # (that represents that word in the Tokenizer that we created before) \n",
    "    \n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f656ef78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[666,\n",
       " 20,\n",
       " 461,\n",
       " 3843,\n",
       " 27,\n",
       " 3844,\n",
       " 4,\n",
       " 61,\n",
       " 23,\n",
       " 1,\n",
       " 3845,\n",
       " 92,\n",
       " 116,\n",
       " 1666,\n",
       " 15,\n",
       " 20,\n",
       " 941,\n",
       " 99,\n",
       " 301,\n",
       " 335,\n",
       " 22,\n",
       " 3,\n",
       " 224,\n",
       " 85,\n",
       " 118,\n",
       " 200,\n",
       " 34,\n",
       " 3,\n",
       " 67,\n",
       " 2,\n",
       " 59,\n",
       " 2,\n",
       " 1667,\n",
       " 225,\n",
       " 6,\n",
       " 35,\n",
       " 96,\n",
       " 10,\n",
       " 48,\n",
       " 76]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now check one sentence:\n",
    "X_train_seq[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59baa70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the list of integers above represent words that occur in that message\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12b1c945",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since machine learning models expect the same number of feature for each example it sees, \n",
    "# --> each SEQuence has to have the SAME LENGTH \n",
    "# using the pad_sequences built-in KERAS function\n",
    "\n",
    "X_train_seq_padded = pad_sequences(X_train_seq, 50)\n",
    "# the 50 stands for the max length each sequence (sentence/document) should have\n",
    "X_test_seq_padded = pad_sequences(X_test_seq, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb3f267b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0,    0,    0,    0,    0,    0,    0,    0,    0,    0,  666,\n",
       "         20,  461, 3843,   27, 3844,    4,   61,   23,    1, 3845,   92,\n",
       "        116, 1666,   15,   20,  941,   99,  301,  335,   22,    3,  224,\n",
       "         85,  118,  200,   34,    3,   67,    2,   59,    2, 1667,  225,\n",
       "          6,   35,   96,   10,   48,   76])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_seq_padded[16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ad4b844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4459, 50)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train_seq_padded.shape)\n",
    "print(type(X_train_seq_padded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58db747a",
   "metadata": {},
   "source": [
    "### Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cf32e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.layers import Dense,Embedding,LSTM\n",
    "from keras.models import Sequential "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "741bbe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    \n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    \n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4ad973b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding (Embedding)        (None, None, 32)          256160    \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 32)                8320      \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 265,569\n",
      "Trainable params: 265,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential() \n",
    "# building the model means that we add as much layer to the sequential model as we think are needed\n",
    "# sequential means that the output dimensionality of one layer is the input d. of the next layer etc.\n",
    "# the first layer will take the dimensionality (SIZE) of our dataset\n",
    "\n",
    "# Embedding works the same way as word2vec and doc2vec\n",
    "model.add(Embedding(len(tokenizer.index_word)+1, 32))\n",
    "model.add(LSTM(32, dropout=0, recurrent_dropout=0))\n",
    "# dropout helps with the regularization\n",
    "    # it prevents overfitting by killing the best neurons so the other would learn better\n",
    "# Dense is a fully-connected layer\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bf82caa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer = 'adam',\n",
    "    loss = 'binary_crossentropy',\n",
    "    metrics = ['accuracy', precision_m, recall_m]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b274f5ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "140/140 [==============================] - 7s 49ms/step - loss: 0.2306 - accuracy: 0.9285 - precision_m: 0.5569 - recall_m: 0.4741 - val_loss: 0.0422 - val_accuracy: 0.9883 - val_precision_m: 0.9848 - val_recall_m: 0.9501\n",
      "Epoch 2/10\n",
      "140/140 [==============================] - 4s 31ms/step - loss: 0.0319 - accuracy: 0.9922 - precision_m: 0.9751 - recall_m: 0.9383 - val_loss: 0.0286 - val_accuracy: 0.9910 - val_precision_m: 0.9735 - val_recall_m: 0.9711\n",
      "Epoch 3/10\n",
      "140/140 [==============================] - 5s 36ms/step - loss: 0.0128 - accuracy: 0.9971 - precision_m: 0.9662 - recall_m: 0.9554 - val_loss: 0.0307 - val_accuracy: 0.9883 - val_precision_m: 0.9456 - val_recall_m: 0.9752\n",
      "Epoch 4/10\n",
      "140/140 [==============================] - 5s 36ms/step - loss: 0.0063 - accuracy: 0.9987 - precision_m: 0.9905 - recall_m: 0.9839 - val_loss: 0.0297 - val_accuracy: 0.9919 - val_precision_m: 0.9871 - val_recall_m: 0.9663\n",
      "Epoch 5/10\n",
      "140/140 [==============================] - 4s 30ms/step - loss: 0.0032 - accuracy: 0.9989 - precision_m: 0.9914 - recall_m: 0.9867 - val_loss: 0.0331 - val_accuracy: 0.9865 - val_precision_m: 0.9314 - val_recall_m: 0.9740\n",
      "Epoch 6/10\n",
      "140/140 [==============================] - 5s 36ms/step - loss: 0.0014 - accuracy: 0.9996 - precision_m: 0.9986 - recall_m: 0.9988 - val_loss: 0.0397 - val_accuracy: 0.9919 - val_precision_m: 1.0000 - val_recall_m: 0.9463\n",
      "Epoch 7/10\n",
      "140/140 [==============================] - 5s 36ms/step - loss: 8.6980e-04 - accuracy: 0.9998 - precision_m: 0.9929 - recall_m: 0.9905 - val_loss: 0.0417 - val_accuracy: 0.9901 - val_precision_m: 1.0000 - val_recall_m: 0.9359\n",
      "Epoch 8/10\n",
      "140/140 [==============================] - 5s 35ms/step - loss: 3.8195e-04 - accuracy: 1.0000 - precision_m: 0.9857 - recall_m: 0.9857 - val_loss: 0.0469 - val_accuracy: 0.9892 - val_precision_m: 0.9833 - val_recall_m: 0.9394\n",
      "Epoch 9/10\n",
      "140/140 [==============================] - 4s 30ms/step - loss: 1.0515e-04 - accuracy: 1.0000 - precision_m: 0.9857 - recall_m: 0.9857 - val_loss: 0.0532 - val_accuracy: 0.9892 - val_precision_m: 1.0000 - val_recall_m: 0.9318\n",
      "Epoch 10/10\n",
      "140/140 [==============================] - 5s 36ms/step - loss: 7.6746e-05 - accuracy: 1.0000 - precision_m: 0.9929 - recall_m: 0.9929 - val_loss: 0.0529 - val_accuracy: 0.9892 - val_precision_m: 0.9929 - val_recall_m: 0.9375\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_seq_padded, \n",
    "                    y_train,\n",
    "                    batch_size=32, \n",
    "                    epochs=10,\n",
    "                    validation_data=(X_test_seq_padded, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da6e85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### visualizing the results data ###\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for i in ['accuracy', 'precision_m', 'recall_m']:\n",
    "    pass\n",
    "\n",
    "\n",
    "# continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
